{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1626484718598,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "v5BV0rcBZh5_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.data import AUTOTUNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ltBCl7IawQrT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3000)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#if gpus:\n",
    "#  try:\n",
    "#    for gpu in gpus:\n",
    "#      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#  except RuntimeError as e:\n",
    "#    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1626484739816,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "C8QwMoKewma5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.metrics as metrics\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "# default input shape\n",
    "INPUT_SHAPE = (512, 1024, 3)\n",
    "\n",
    "\n",
    "def ge_layer(x_in, c, e=6, stride=1):\n",
    "    x = layers.Conv2D(filters=c, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), padding='same')(x_in)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    if stride == 2:\n",
    "        x = layers.DepthwiseConv2D(depth_multiplier=e, kernel_size=(3,3), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        y = layers.DepthwiseConv2D(depth_multiplier=e, kernel_size=(3,3), strides=2, padding='same')(x_in)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Conv2D(filters=c, kernel_size=(1,1),kernel_regularizer=regularizers.l2(5e-4), padding='same')(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "    else:\n",
    "        y = x_in\n",
    "        \n",
    "    x = layers.DepthwiseConv2D(depth_multiplier=e, kernel_size=(3,3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(filters=c, kernel_size=(1,1),kernel_regularizer=regularizers.l2(5e-4), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Add()([x, y])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def stem(x_in, c):\n",
    "    x = layers.Conv2D(filters=c, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), strides=2, padding='same')(x_in)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x_split = layers.Activation('relu')(x)\n",
    "    \n",
    "    #x = layers.MaxPool2D(pool_size=(1, 1), padding='same')(x_split)\n",
    "    x = layers.Conv2D(filters=c // 2, kernel_size=(1,1),kernel_regularizer=regularizers.l2(5e-4), padding='same')(x_split)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters=c, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    y = layers.MaxPooling2D()(x_split)\n",
    "    \n",
    "    x = layers.Concatenate()([x, y])\n",
    "    x = layers.Conv2D(filters=c, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def detail_conv2d(x_in, c, stride=1):\n",
    "    x = layers.Conv2D(filters=c, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), strides=stride, padding='same')(x_in)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def context_embedding(x_in, c):\n",
    "    x = layers.GlobalAveragePooling2D()(x_in)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Reshape((1,1,c))(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters=c, kernel_size=(1,1),kernel_regularizer=regularizers.l2(5e-4), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    # broadcasting no needed\n",
    "    \n",
    "    x = layers.Add()([x, x_in])\n",
    "    x = layers.Conv2D(filters=c, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), padding='same')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def bilateral_guided_aggregation(detail, semantic, c):\n",
    "    # detail branch\n",
    "    detail_a = layers.DepthwiseConv2D(kernel_size=(3,3), padding='same')(detail)\n",
    "    detail_a = layers.BatchNormalization()(detail_a)\n",
    "    \n",
    "    detail_a = layers.Conv2D(filters=c, kernel_size=(1,1),kernel_regularizer=regularizers.l2(5e-4), padding='same')(detail_a)\n",
    "    \n",
    "    #detail_a = layers.MaxPool2D(pool_size=(3,3), strides=2, padding='same')(detail_a)\n",
    "    \n",
    "    detail_b = layers.Conv2D(filters=c, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), strides=2, padding='same')(detail)\n",
    "    detail_b = layers.BatchNormalization()(detail_b)\n",
    "    \n",
    "    detail_b = layers.AveragePooling2D((3,3), strides=2, padding='same')(detail_b)\n",
    "    \n",
    "    # semantic branch\n",
    "    semantic_a = layers.DepthwiseConv2D(kernel_size=(3,3), padding='same')(semantic)\n",
    "    semantic_a = layers.BatchNormalization()(semantic_a)\n",
    "    \n",
    "    #semantic_a = layers.MaxPool2D(pool_size=(1, 1), padding='same')(semantic_a)\n",
    "    semantic_a = layers.Conv2D(filters=c, kernel_size=(1,1),kernel_regularizer=regularizers.l2(5e-4), padding='same')(semantic_a)\n",
    "    semantic_a = layers.Activation('sigmoid')(semantic_a)\n",
    "    \n",
    "    semantic_b = layers.Conv2D(filters=c, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), padding='same')(semantic)\n",
    "    semantic_b = layers.BatchNormalization()(semantic_b)\n",
    "    \n",
    "    #semantic_b = layers.MaxPool2D(pool_size=(3,3), strides=2, padding='same')(semantic_b)\n",
    "    \n",
    "    semantic_b = layers.UpSampling2D((4,4), interpolation='bilinear')(semantic_b)\n",
    "    semantic_b = layers.Activation('sigmoid')(semantic_b)\n",
    "    \n",
    "    # combining\n",
    "    detail = layers.Multiply()([detail_a, semantic_b])\n",
    "    semantic = layers.Multiply()([semantic_a, detail_b])\n",
    "    \n",
    "    # this layer is not mentioned in the paper !?\n",
    "    #semantic = layers.UpSampling2D((4,4))(semantic)\n",
    "    semantic = layers.UpSampling2D((4,4), interpolation='bilinear')(semantic)\n",
    "    \n",
    "    x = layers.Add()([detail, semantic])\n",
    "    x = layers.Conv2D(filters=c, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def seg_head(x_in, c_t, s, n):\n",
    "    x = layers.Conv2D(filters=c_t, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), padding='same')(x_in)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters=n, kernel_size=(3,3),kernel_regularizer=regularizers.l2(5e-4), padding='same')(x)\n",
    "    x = layers.UpSampling2D((s,s), interpolation='bilinear')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "class ArgmaxMeanIOU(metrics.MeanIoU):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        return super().update_state(tf.argmax(y_true, axis=-1), tf.argmax(y_pred, axis=-1), sample_weight)\n",
    "\n",
    "\n",
    "def bisenetv2(num_classes=2, out_scale=8, input_shape=INPUT_SHAPE, l=4, seghead_expand_ratio=2):\n",
    "    x_in = layers.Input(input_shape)\n",
    "\n",
    "    # semantic branch\n",
    "    # S1 + S2\n",
    "    x = stem(x_in, 16 // 1)\n",
    "    \n",
    "    # S3\n",
    "    x = ge_layer(x, 32 // 1, stride=2)\n",
    "    x = ge_layer(x, 32 // 1, stride=1)\n",
    "\n",
    "    # S4\n",
    "    #x = ge_layer(x, 64, stride=2)\n",
    "    #x = ge_layer(x, 64, stride=1)\n",
    "\n",
    "    # S5\n",
    "    #x = ge_layer(x, 128, stride=2)\n",
    "\n",
    "    #x = ge_layer(x, 128, stride=1)\n",
    "    #x = ge_layer(x, 128, stride=1)\n",
    "    #x = ge_layer(x, 128, stride=1)\n",
    "\n",
    "    #x = context_embedding(x, 128)\n",
    "    x = context_embedding(x, 32)\n",
    "\n",
    "    # detail branch\n",
    "    # S1\n",
    "    y = detail_conv2d(x_in, 64, stride=2)\n",
    "    y = detail_conv2d(y, 64, stride=1)\n",
    "\n",
    "    # S2\n",
    "    #y = detail_conv2d(y, 64, stride=2)\n",
    "    #y = detail_conv2d(y, 64, stride=1)\n",
    "    #y = detail_conv2d(y, 64, stride=1)\n",
    "\n",
    "    # S3\n",
    "    #y = detail_conv2d(y, 128, stride=2)\n",
    "    #y = detail_conv2d(y, 128, stride=1)\n",
    "    #y = detail_conv2d(y, 128, stride=1)\n",
    "\n",
    "    #x = bilateral_guided_aggregation(y, x, 128)\n",
    "    x = bilateral_guided_aggregation(y, x, 64)\n",
    "\n",
    "    x = seg_head(x, num_classes * seghead_expand_ratio, out_scale, num_classes)\n",
    "    \n",
    "    model = models.Model(inputs=[x_in], outputs=[x])\n",
    "    \n",
    "    # set weight initializers\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel_initializer = tf.keras.initializers.HeNormal()\n",
    "        if hasattr(layer, 'depthwise_initializer'):\n",
    "            layer.depthwise_initializer = tf.keras.initializers.HeNormal()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def bisenetv2_compiled(num_classes, decay_steps=5e4, momentum=0.9, weight_decay=0.0005, **kwargs):\n",
    "    model = bisenetv2(num_classes, **kwargs)\n",
    "\n",
    "    schedule = optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=5e-2,\n",
    "        decay_steps=decay_steps,\n",
    "        power=0.9\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        import tensorflow_addons as tfa\n",
    "\n",
    "        sgd = tfa.optimizers.SGDW(\n",
    "            weight_decay=weight_decay,\n",
    "            learning_rate=schedule,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "    except ImportError:\n",
    "        print('tensorflow_addons not available, not using weight-decay')\n",
    "\n",
    "        sgd = optimizers.SGD(\n",
    "            learning_rate=schedule,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "\n",
    "    cce = losses.CategoricalCrossentropy(from_logits=True)\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    model.compile(sgd, loss=loss,\n",
    "                  metrics=['accuracy', ArgmaxMeanIOU(num_classes)]) \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def bisenetv2_output_shape(num_classes, scale, input_shape=INPUT_SHAPE):\n",
    "    return ((input_shape[0] // 8) * scale, \n",
    "            (input_shape[1] // 8) * scale, \n",
    "            num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1336,
     "status": "ok",
     "timestamp": 1626484749450,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "vASFxoROOOI7"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def class_map_road(seg):\n",
    "    # map class 0=anything, 1=road\n",
    "    return tf.where(seg == 7, [0, 1.0], [1.0, 0])\n",
    "\n",
    "\n",
    "def cityscapes_prep(output_shape, input_shape=INPUT_SHAPE, class_map_func=None, float_range=True, standardize=True):\n",
    "    def prep_map(sample):\n",
    "        img = sample['image_left']\n",
    "        seg = sample['segmentation_label']\n",
    "\n",
    "        if float_range:\n",
    "            img /= 255\n",
    "        if standardize:\n",
    "            img = tf.image.per_image_standardization(img)\n",
    "\n",
    "        img = tf.image.resize(img, input_shape[0:2])\n",
    "        seg = tf.image.resize(seg, output_shape[0:2])\n",
    "        \n",
    "        if callable(class_map_func):\n",
    "            seg = class_map_func(seg)\n",
    "        else:\n",
    "            seg = tf.one_hot(tf.cast(seg, tf.int32), output_shape[-1], axis=2)\n",
    "            seg = tf.cast(seg, tf.float32)\n",
    "            seg = tf.squeeze(seg)\n",
    "            #seg = tf.keras.utils.to_categorical(seg, num_classes=output_shape[-1])\n",
    "\n",
    "        return img, seg\n",
    "\n",
    "    return prep_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1626484753076,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "W257X3Z9EalC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def segmentation_to_image(pred):\n",
    "    img = tf.argmax(pred, axis=-1)\n",
    "    img = img[..., tf.newaxis]\n",
    "    return tf.keras.preprocessing.image.array_to_img(img)\n",
    "\n",
    "        \n",
    "def predict_tf(model):\n",
    "    def predict_func(sample):\n",
    "        print(sample[0])\n",
    "        pred = model.predict(tf.expand_dims(sample[0], axis=0))\n",
    "        return sample[0], pred[0]\n",
    "    \n",
    "    return predict_func\n",
    "\n",
    "\n",
    "def display_dataset(ds, pred_func):\n",
    "    for sample in ds:\n",
    "        imgs = pred_func(sample)\n",
    "        fig, axes = plt.subplots(1, len(imgs))\n",
    "        \n",
    "        for ax, img in zip(axes, imgs):\n",
    "            if img.shape[-1] != 3:\n",
    "                img = segmentation_to_image(img)\n",
    "                \n",
    "            ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1626484756568,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "v05dFZqSjMLH"
   },
   "outputs": [],
   "source": [
    "# Test a image input\n",
    "import cv2\n",
    "import time\n",
    "def img_pred(src,model,image_size):\n",
    "  src = cv2.cvtColor(src, cv2.COLOR_BGR2RGB)\n",
    "  #cv2_imshow(src)\n",
    "  image = cv2.resize(src,image_size,interpolation=cv2.INTER_CUBIC)\n",
    "  #cv2_imshow(src)\n",
    "  image = image/255\n",
    "  data = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "  data = tf.expand_dims(data, axis=0)\n",
    "  pred = model.predict(data)\n",
    "  seg = tf.argmax(pred[0], axis=-1)\n",
    "  seg = seg[..., tf.newaxis]\n",
    "  seg = tf.keras.preprocessing.image.array_to_img(seg)\n",
    "  #plt.imshow(seg)\n",
    "  seg = cv2.cvtColor(np.array(seg), cv2.COLOR_BGR2RGB)\n",
    "  seg = cv2.resize(seg,(src.shape[1],src.shape[0]),interpolation=cv2.INTER_CUBIC)\n",
    "  result = cv2.addWeighted(np.array(src), 0.6, seg, 0.5, 0, dtype = cv2.CV_8U)\n",
    "  return src, seg, result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1626484759302,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "OqSE3ejVkoXh"
   },
   "outputs": [],
   "source": [
    "def gen_seg_vid(cap,out,shape,model):\n",
    "  #while(cap.isOpened()):\n",
    "  for _ in range(500):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "      start = time.time()\n",
    "      img_seg = img_pred(frame,model,(256,128))[2]\n",
    "      end = time.time()\n",
    "      fps = 1/(end - start)\n",
    "      x = 'FPS:'\n",
    "      text = \"{}{:.3f}\".format(x,fps)\n",
    "      font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "      #cv2_imshow(img_seg)\n",
    "      img_seg = cv2.cvtColor(img_seg, cv2.COLOR_BGR2RGB)\n",
    "      cv2.putText(img_seg, text , (800,900), font, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "      out.write(img_seg)\n",
    "      if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        break\n",
    "    else:\n",
    "      break\n",
    "  cap.release()\n",
    "  out.release()\n",
    "  cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1626484763979,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "GNenks0SZh6D"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (128, 256, 3)\n",
    "NUM_CLASSES = 2\n",
    "SCALE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1626484764965,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "bpIG9PvNZh6D",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_SHAPE = bisenetv2_output_shape(NUM_CLASSES, SCALE, input_shape=INPUT_SHAPE)\n",
    "m = bisenetv2_compiled(num_classes=NUM_CLASSES, out_scale=SCALE, input_shape=INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1626429399003,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "mkvcgUGQZh6D",
    "outputId": "41501700-e86b-4da5-c067-e8dfb0aa894b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "#model = tf.keras.models.load_model('E:/EE/project/FPGA/128x256/bisenet_small_aug.tf',custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU}, compile=False)\n",
    "\n",
    "\n",
    "decay_steps=5e4\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "\n",
    "schedule = optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=1e-2,\n",
    "        decay_steps=decay_steps,\n",
    "        power=0.9\n",
    "    )\n",
    "\n",
    "schedule_cosine= optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate = 1e-2, decay_steps=decay_steps, alpha=0.0)\n",
    "\n",
    "\n",
    "sgd = tfa.optimizers.SGDW(\n",
    "            weight_decay=weight_decay,\n",
    "            learning_rate=schedule,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "cce = losses.CategoricalCrossentropy(from_logits=True)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "yogi =  tfa.optimizers.Yogi(learning_rate = 0.0005)\n",
    "adamw = tfa.optimizers.AdamW(learning_rate=1e-5 , weight_decay=0.0005)\n",
    "swa = tfa.optimizers.SWA(adamw)\n",
    "\n",
    "m.compile(adam, loss=cce,\n",
    "                  metrics=['accuracy', ArgmaxMeanIOU(NUM_CLASSES)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OUTPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GFCR61BZh6E"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(m, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4510,
     "status": "ok",
     "timestamp": 1626484772545,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "2rmMwzroZh6F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cityscapes = tfds.load('cityscapes/semantic_segmentation',data_dir=\"E:\\EE\\project\\FPGA\\cityscapes\",download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1626484772546,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "7hOcLzacZh6G"
   },
   "outputs": [],
   "source": [
    "OUTPUT_SHAPE = (128, 256, 2)\n",
    "train_ds = cityscapes['train'].map(cityscapes_prep(OUTPUT_SHAPE, INPUT_SHAPE, class_map_road))\n",
    "valid_ds = cityscapes['validation'].map(cityscapes_prep(OUTPUT_SHAPE, INPUT_SHAPE, class_map_road))\n",
    "test_ds = cityscapes['test'].map(cityscapes_prep(OUTPUT_SHAPE, INPUT_SHAPE, class_map_road))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((128, 256, 3), (128, 256, 2)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image,label = next(iter(zoom_ds))\n",
    "#level = np.round(np.random.uniform(-0.2, 0.2), 3)\n",
    "#print(level)\n",
    "#print(label)\n",
    "#image = tfa.image.shear_y(image, level, tf.constant(0))\n",
    "#_ = plt.imshow(image)\n",
    "seg = tf.argmax(label, axis=-1)\n",
    "seg = seg[..., tf.newaxis]\n",
    "seg = tf.keras.preprocessing.image.array_to_img(seg)\n",
    "plt.imshow(seg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "\n",
    "Crop_img = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomCrop(128, 256, seed=seed)\n",
    "])\n",
    "Crop_seg= tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomCrop(128, 256, seed=seed)\n",
    "])\n",
    "Rotate_img = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomRotation(0.06, fill_mode='constant', seed = seed)\n",
    "])\n",
    "Rotate_seg = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomRotation(0.06, fill_mode='constant', seed = seed)\n",
    "])\n",
    "Zoom_img = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomZoom((-0.75,0.25), fill_mode='constant', seed = seed)\n",
    "])\n",
    "Zoom_seg = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomZoom((-0.75,0.25), fill_mode='constant', seed = seed)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"SelectV2:0\", shape=(128, 256, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def zoom_prep(output_shape, input_shape=INPUT_SHAPE, class_map_func=None, float_range=True, standardize=True):\n",
    "    def prep_map(sample):\n",
    "        img = sample['image_left']\n",
    "        seg = sample['segmentation_label']\n",
    "\n",
    "        if float_range:\n",
    "            img /= 255\n",
    "        if standardize:\n",
    "            img = tf.image.per_image_standardization(img)\n",
    "\n",
    "        img = tf.image.resize(img, input_shape[0:2])\n",
    "        seg = tf.image.resize(seg, output_shape[0:2])\n",
    "        \n",
    "        img = img[None, ...]\n",
    "        seg = seg[None, ...]\n",
    "        img = Zoom_img(img)\n",
    "        seg = Zoom_seg(seg)\n",
    "        img = tf.squeeze(img)\n",
    "        seg = tf.squeeze(seg,0)\n",
    "        \n",
    "        if callable(class_map_func):\n",
    "            seg = class_map_func(seg)\n",
    "        else:\n",
    "            seg = tf.one_hot(tf.cast(seg, tf.int32), output_shape[-1], axis=2)\n",
    "            seg = tf.cast(seg, tf.float32)\n",
    "            seg = tf.squeeze(seg)\n",
    "            #seg = tf.keras.utils.to_categorical(seg, num_classes=output_shape[-1])\n",
    "        print(seg)\n",
    "        return img, seg\n",
    "\n",
    "    return prep_map\n",
    "\n",
    "OUTPUT_SHAPE = (128, 256, 2)\n",
    "INPUT_SHAPE = (128, 256, 3)\n",
    "\n",
    "zoom_ds = cityscapes['train'].map(zoom_prep(OUTPUT_SHAPE, INPUT_SHAPE, class_map_road))\n",
    "zoom_ds_flipped = zoom_ds.shuffle(500).take(np.round(len(zoom_ds)/2)).map(lambda img, seg: (tf.image.flip_left_right(img), tf.image.flip_left_right(seg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crop_prep(output_shape, input_shape=INPUT_SHAPE, class_map_func=None, float_range=True, standardize=True):\n",
    "    def prep_map(sample):\n",
    "        img = sample['image_left']\n",
    "        seg = sample['segmentation_label']\n",
    "\n",
    "        if float_range:\n",
    "            img /= 255\n",
    "        if standardize:\n",
    "            img = tf.image.per_image_standardization(img)\n",
    "\n",
    "        img = tf.image.resize(img, input_shape[0:2])\n",
    "        seg = tf.image.resize(seg, output_shape[0:2])\n",
    "        \n",
    "        img = img[None, ...]\n",
    "        seg = seg[None, ...]\n",
    "        img = Crop_img(img)\n",
    "        seg = Crop_seg(seg)\n",
    "        img = tf.squeeze(img)\n",
    "        seg = tf.squeeze(seg,0)\n",
    "        \n",
    "        if callable(class_map_func):\n",
    "            seg = class_map_func(seg)\n",
    "        else:\n",
    "            seg = tf.one_hot(tf.cast(seg, tf.int32), output_shape[-1], axis=2)\n",
    "            seg = tf.cast(seg, tf.float32)\n",
    "            seg = tf.squeeze(seg)\n",
    "            #seg = tf.keras.utils.to_categorical(seg, num_classes=output_shape[-1])\n",
    "        \n",
    "        return img, seg\n",
    "\n",
    "    return prep_map\n",
    "\n",
    "input_shape = (200,400,3)\n",
    "output_shape = (200,400,2)\n",
    "crop_ds = cityscapes['train']\n",
    "\n",
    "crop_ds = crop_ds.map(Crop_prep(output_shape, input_shape, class_map_road))\n",
    "crop_ds_flipped = crop_ds.shuffle(500).take(np.round(len(crop_ds)/2)).map(lambda img, seg: (tf.image.flip_left_right(img), tf.image.flip_left_right(seg)))\n",
    "\n",
    "#display_dataset(tmp_ds.take(1), lambda s: (s[0], s[1]))\n",
    "#display_dataset(crop_ds_flipped.take(5), lambda s: (s[0], s[1]))\n",
    "#display_dataset(crop_ds.take(5), lambda s: (s[0], s[1]))\n",
    "#plt.show()\n",
    "\n",
    "aug_ds = crop_ds_flipped.concatenate(zoom_ds_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(aug_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1626484777849,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "WSQCFV8wZh6G",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_1:0\", shape=(128, 256, 2), dtype=float32)\n",
      "Tensor(\"args_1:0\", shape=(128, 256, 2), dtype=float32)\n",
      "Tensor(\"args_1:0\", shape=(128, 256, 2), dtype=float32)\n",
      "Tensor(\"args_1:0\", shape=(128, 256, 2), dtype=float32)\n",
      "Tensor(\"args_1:0\", shape=(128, 256, 2), dtype=float32)\n",
      "Tensor(\"args_1:0\", shape=(128, 256, 2), dtype=float32)\n",
      "Tensor(\"args_1:0\", shape=(128, 256, 2), dtype=float32)\n",
      "<ConcatenateDataset shapes: ((128, 256, 3), (128, 256, 2)), types: (tf.float32, tf.float32)>\n",
      "<PrefetchDataset shapes: ((None, 128, 256, 3), (None, 128, 256, 2)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "USE_TINYCAR_DATA = False\n",
    "USE_AUGMENTATION = True\n",
    "\n",
    "rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "\n",
    "if USE_TINYCAR_DATA:\n",
    "    from data_prep import labelme_prep, uwula_prep\n",
    "\n",
    "    ds = tf.data.Dataset.list_files('../paperstreet/1/*.json')\n",
    "    ds = ds.map(labelme_prep(OUTPUT_SHAPE, INPUT_SHAPE))\n",
    "\n",
    "\n",
    "    ds2 = tf.data.Dataset.list_files('/media/mldata/tinycar/Mikrowunderland_1k/*.jpg')\n",
    "    ds2 = ds2.map(uwula_prep(OUTPUT_SHAPE, INPUT_SHAPE))\n",
    "\n",
    "    ds = ds.concatenate(ds2)\n",
    "    \n",
    "    tc_valid_ds = ds.take(24)\n",
    "    tc_train_ds = ds.skip(24)\n",
    "    \n",
    "    train_ds = train_ds.concatenate(tc_train_ds)\n",
    "    valid_ds = train_ds.concatenate(tc_valid_ds)\n",
    "\n",
    "if USE_AUGMENTATION:\n",
    "\n",
    "    def augment(img, seg, seed):\n",
    "        #seed = tf.random.uniform((2,))\n",
    "        print(seg)\n",
    "        img = tf.image.stateless_random_brightness(img, 0.2, seed=seed)\n",
    "        img = tf.image.stateless_random_contrast(img, 0.8, 1.2, seed=seed)\n",
    "        img = tf.image.stateless_random_saturation(img, 0.0, 1.5, seed=seed)\n",
    "        img = tf.image.stateless_random_hue(img, 0.2, seed=seed)\n",
    "        img = tf.clip_by_value(img, 0.0, 1.0) # clip values outside 0..1\n",
    "\n",
    "        return img, seg\n",
    "\n",
    "    def randomize(img, seg):\n",
    "        seed = rng.make_seeds(2)[0]\n",
    "        img, seg = augment(img, seg, seed)\n",
    "        return img, seg\n",
    "    \n",
    "    train_ds_flipped_lr = train_ds.map(lambda img, seg: (tf.image.flip_left_right(img), tf.image.flip_left_right(seg)))\n",
    "    valid_ds_flipped_lr = valid_ds.map(lambda img, seg: (tf.image.flip_left_right(img), tf.image.flip_left_right(seg)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_ds_randomized = train_ds.map(randomize)\n",
    "    #train_ds_shear = train_ds.map(shear)\n",
    "    crop_ds_randomized = crop_ds.map(randomize)\n",
    "    zoom_ds_randomized = zoom_ds.map(randomize)\n",
    "    valid_ds_randomized = valid_ds.map(randomize)\n",
    "    \n",
    "    train_ds_randomized_flipped = train_ds_flipped_lr.map(randomize)\n",
    "    valid_ds_randomized_flipped = valid_ds_flipped_lr.map(randomize)\n",
    "    aug_ds_rand = aug_ds.map(randomize)\n",
    "    \n",
    "    train_ds = train_ds.concatenate(train_ds_flipped_lr).concatenate(train_ds_randomized).concatenate(train_ds_randomized_flipped)\n",
    "    valid_ds = valid_ds.concatenate(valid_ds_flipped_lr).concatenate(valid_ds_randomized).concatenate(valid_ds_randomized_flipped)\n",
    "\n",
    "    #train_ds = train_ds.concatenate(train_ds_flipped_lr).concatenate(train_ds_randomized).concatenate(crop_ds_flipped).concatenate(train_ds_randomized).concatenate(zoom_ds_flipped)\n",
    "    \n",
    "    #train_ds = train_ds.concatenate(aug_ds).concatenate(train_ds_randomized).concatenate(aug_ds_rand)\n",
    "\n",
    "print(train_ds)\n",
    "train_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "valid_ds = valid_ds.batch(4).prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "print(valid_ds)\n",
    "#display_dataset(valid_ds.unbatch().take(1), lambda s: (s[0], s[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import Flatten, Reshape\n",
    "def DiceLoss(targets, inputs, smooth=1e-6):\n",
    "    \n",
    "    #flatten label and prediction tensors\n",
    "    #inputs = Flatten()(inputs)\n",
    "    #targets = Flatten()(targets)\n",
    "    targets = tf.argmax(targets, axis=-1)\n",
    "    targets = targets[..., tf.newaxis]\n",
    "    inputs = tf.argmax(inputs, axis=-1)\n",
    "    inputs = inputs[..., tf.newaxis]\n",
    "    targets = tf.cast(targets, tf.float32)\n",
    "    inputs = tf.cast(inputs, tf.float32)\n",
    "    \n",
    "    print(targets, inputs)\n",
    "    intersection = K.sum(targets * inputs, axis=[1,2])\n",
    "    union = K.sum(targets, axis=[1,2,3]) + K.sum(inputs, axis=[1,2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    dice = K.mean(dice,axis=0)\n",
    "    dice = K.mean(dice)\n",
    "    return 1 - dice\n",
    "\n",
    "\n",
    "def DiceCCELoss(targets, inputs, smooth=1e-6):    \n",
    "    \n",
    "    #flatten label and prediction tensors\n",
    "    I = inputs\n",
    "    T = targets\n",
    "    #inputs = Flatten()(inputs)\n",
    "    inputs = Reshape((-1,))(inputs)\n",
    "    #targets = Flatten()(targets)\n",
    "    targets = Reshape((-1,))(targets)\n",
    "    #print(inputs)\n",
    "    #print(targets)\n",
    "    #print(I,T)\n",
    "    print(targets,inputs)\n",
    "    CCE =  K.categorical_crossentropy(targets, inputs, from_logits = True)\n",
    "    targets = T\n",
    "    inpits = I\n",
    "    #intersection = K.sum(targets*inputs, axis=(1,2,3))\n",
    "    #union = K.sum(targets, axis=(1,2,3)) + K.sum(inputs, axis=(1,2,3))\n",
    "    intersection = K.sum(targets * inputs, axis=[1,2,3])\n",
    "    total = K.sum(targets, axis=[1,2,3]) + K.sum(inputs, axis=[1,2,3])\n",
    "    union = total - intersection\n",
    "    dice = k.mean((2. * intersection + smooth) / (union + smooth),axis = 0)\n",
    "    dice_loss = 1 - dice  \n",
    "    Dice_CCE = CCE + dice_loss\n",
    "    #IoU = IoULoss(targets,inputs,1e-5)\n",
    "    #Loss = CCE + IoU\n",
    "    \n",
    "    return Dice_CCE\n",
    "\n",
    "def IoULoss(targets, inputs, smooth=1e-6):\n",
    "    \n",
    "    #flatten label and prediction tensors\n",
    "    #inputs = Flatten()(inputs)\n",
    "    #targets = Flatten()(targets)\n",
    "    \n",
    "    #intersection = K.sum(K.dot(targets, inputs))\n",
    "    intersection = K.sum(targets * inputs, axis=[1,2])\n",
    "    #total = K.sum(targets) + K.sum(inputs)\n",
    "    total = K.sum(targets, axis=[1,2]) + K.sum(inputs, axis=[1,2])\n",
    "    union = total - intersection\n",
    "    IoU = K.mean((intersection + smooth) / (union + smooth), axis = 0)\n",
    "    #IoU = (intersection + smooth)/(union + smooth)\n",
    "    #return 1 - IoU\n",
    "    return 1 - K.mean(IoU)\n",
    "\n",
    "def CCE(targets, inputs):\n",
    "    return K.categorical_crossentropy(targets, inputs, from_logits = True)\n",
    "\n",
    "def GIoU_CCE(targets,inputs):\n",
    "    print(targets,inputs)\n",
    "    giou = tfa.losses.giou_loss(targets,inputs)\n",
    "    cce = losses.categorical_crossentropy(targets, inputs, from_logits = True)\n",
    "    return giou + cce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phkgaenRZh6H"
   },
   "outputs": [],
   "source": [
    "#train_ds = train_ds.shuffle(500)\n",
    "print(len(valid_ds))\n",
    "#display_dataset(train_ds_randomized_flipped.take(1), lambda s: (s[0], s[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# warm-up training\n",
    "import tensorflow_addons as tfa\n",
    "from typing import Callable\n",
    "\n",
    "class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(\n",
    "    self,\n",
    "    initial_learning_rate: float,\n",
    "    decay_schedule_fn: Callable,\n",
    "    warmup_steps: int,\n",
    "    power: float = 1.0,\n",
    "    name: str = None,\n",
    "):\n",
    "        super().__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.power = power\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"WarmUp\") as name:\n",
    "            # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n",
    "            # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "            global_step_float = tf.cast(step, tf.float32)\n",
    "            warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "            warmup_percent_done = global_step_float / warmup_steps_float\n",
    "            warmup_learning_rate = (self.initial_learning_rate) * tf.math.pow(warmup_percent_done, self.power)\n",
    "            return tf.cond(\n",
    "                global_step_float < warmup_steps_float,\n",
    "                lambda: warmup_learning_rate,\n",
    "                lambda: self.decay_schedule_fn(step - self.warmup_steps),\n",
    "                name=name,\n",
    "            )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"decay_schedule_fn\": self.decay_schedule_fn,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"power\": self.power,\n",
    "            \"name\": self.name,\n",
    "        }\n",
    "    \n",
    "decay_steps=5e4\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "initial_lr = 5e-2\n",
    "schedule = optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=5e-2,\n",
    "        decay_steps=decay_steps,\n",
    "        power=0.9\n",
    "    )\n",
    "schedule_cosine= optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate = 5e-2, decay_steps=decay_steps, alpha=0.0)\n",
    "\n",
    "lr_schedule = WarmUp(\n",
    "            initial_learning_rate=initial_lr,\n",
    "            decay_schedule_fn=schedule,\n",
    "            warmup_steps=744\n",
    "        )\n",
    "\n",
    "cce = losses.CategoricalCrossentropy(from_logits=True)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "giou = tfa.losses.GIoULoss()\n",
    "\n",
    "adamw = tfa.optimizers.AdamW(learning_rate=lr_schedule , weight_decay=weight_decay)\n",
    "sgdw = tfa.optimizers.SGDW(\n",
    "            weight_decay=weight_decay,\n",
    "            learning_rate=lr_schedule,\n",
    "            momentum=momentum\n",
    "        )\n",
    "sgd = optimizers.SGD(\n",
    "            learning_rate=lr_schedule,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "\n",
    "#m.compile(sgd ,loss =IoULoss ,metrics=['accuracy', ArgmaxMeanIOU(NUM_CLASSES)] )\n",
    "\n",
    "#EPOCHS = 2\n",
    "\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#history = m.fit(train_ds,\n",
    "#                validation_data=valid_ds,\n",
    "#                epochs=EPOCHS,\n",
    "#                callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVsV74F6Zh6I",
    "outputId": "d2bc9452-fdd9-4f52-a32f-37927b672e49",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_copy = tf.keras.models.clone_model(m)\n",
    "model_copy.set_weights(m.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Formal Training\n",
    "\n",
    "\n",
    "decay_steps=5e4\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "\n",
    "schedule_cosine= optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate = 5e-2, decay_steps=decay_steps, alpha=0.0)\n",
    "\n",
    "schedule = optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=5e-2,\n",
    "        decay_steps=decay_steps,\n",
    "        power=0.9,\n",
    "        cycle = True\n",
    "    )\n",
    "lr_schedule = WarmUp(\n",
    "            initial_learning_rate=initial_lr,\n",
    "            decay_schedule_fn=schedule_cosine,\n",
    "            warmup_steps=1500,\n",
    "        )\n",
    "\n",
    "sgdw = tfa.optimizers.SGDW(\n",
    "            weight_decay=weight_decay,\n",
    "            learning_rate=lr_schedule,\n",
    "            momentum=momentum\n",
    "        )\n",
    "\n",
    "sgd = optimizers.SGD(\n",
    "            learning_rate=lr_schedule,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "\n",
    "opt = tfa.optimizers.MovingAverage(sgd)\n",
    "\n",
    "m.compile(opt ,loss = cce , metrics=['accuracy', ArgmaxMeanIOU(NUM_CLASSES)] )\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = m.fit(train_ds,\n",
    "                validation_data=valid_ds,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3GoHXWkZh6I",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#m_copy.save('E:/EE/project/FPGA/128x256/bisenet_small_aug.tf')\n",
    "m.save('E:/EE/project/FPGA/128x256/model_untrained.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgZDEyrXZh6J"
   },
   "outputs": [],
   "source": [
    "#m_copy.compile(sgd, loss = cce, metrics=['accuracy', ArgmaxMeanIOU(NUM_CLASSES)] )\n",
    "model = tf.keras.models.load_model('E:/EE/project/FPGA/128x256/bisenet_small_2.tf',custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU})\n",
    "OUTPUT_SHAPE = (128, 256, 2)\n",
    "eval_ds = cityscapes['test'].map(cityscapes_prep(OUTPUT_SHAPE, INPUT_SHAPE, class_map_road))\n",
    "eval_ds = eval_ds.batch(16)\n",
    "print(\"Evaluate on valid data\")\n",
    "results = model.evaluate(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHuvHdnQZh6J"
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "#display_dataset(test_ds.unbatch().take(1), predict_tf(m))\n",
    "#display_dataset(valid_ds.unbatch().take(20), predict_tf(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('E:/EE/project/FPGA/128x256/stripped_pruned_small_model.tf',custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU}, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hv3M40gvqqqv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Image\n",
    "src = cv2.imread('E:/EE/project/FPGA/test_01.png')\n",
    "start = time.time()\n",
    "result = img_pred(src,pruned_model,(256,128))[2]\n",
    "end = time.time()\n",
    "fps = 1/(end-start)\n",
    "print('fps =',fps)\n",
    "cv2.imshow(\"result\",result)\n",
    "cv2.waitKey (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXLmFuFAUIR1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('E:\\EE\\project\\FPGA\\Driving_data.mp4')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "out = cv2.VideoWriter('E:/EE/project/FPGA/128x256/Dirve_test_P_small_2_GPU.mp4',fourcc,10,(1920,1080))\n",
    "gen_seg_vid(cap,out,(1920,1080),stripped_pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1626489793762,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "6nXoKAh3u8Od"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "decay_steps=10e3\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "\n",
    "schedule = optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=5e-5,\n",
    "        decay_steps=decay_steps,\n",
    "        power=0.9\n",
    "    )\n",
    "sgd = tfa.optimizers.SGDW(\n",
    "            weight_decay=weight_decay,\n",
    "            learning_rate=schedule,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "cce = losses.CategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "#model = tf.keras.models.load_model('E:/EE/project/FPGA/128x256/original_model.tf',custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU}, compile=False)\n",
    "\n",
    "\n",
    "decay_steps=10e3\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "\n",
    "schedule = optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=5e-2,\n",
    "        decay_steps=decay_steps,\n",
    "        power=0.9\n",
    "    )\n",
    "\n",
    "\n",
    "sgd = tfa.optimizers.SGDW(\n",
    "            weight_decay=weight_decay,\n",
    "            learning_rate=schedule,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "cce = losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "#model.compile(sgd, loss=cce,\n",
    "#                  metrics=['accuracy', ArgmaxMeanIOU(NUM_CLASSES)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70860,
     "status": "ok",
     "timestamp": 1626489866597,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "1Y4DPagSqZzq",
    "outputId": "153a8021-1637-4b64-ba7f-4ded228b87b7"
   },
   "outputs": [],
   "source": [
    "# Prune the original TF model\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('E:/EE/project/FPGA/128x256/bisenet_small_2.tf',custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU})\n",
    "\n",
    "#model = m \n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "epochs = 10\n",
    "end_step = np.ceil(len(train_ds)).astype(np.int32)*epochs\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.5,\n",
    "                                    final_sparsity=0.7,\n",
    "                                    begin_step=0,\n",
    "                                    end_step=end_step)}\n",
    " #     'pruning_policy': tfmot.sparsity.keras.PruneForLatencyOnXNNPack()}\n",
    "#pruning_params = {\n",
    "#      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.7, begin_step=0, frequency=100)\n",
    "#  }\n",
    "# Try to apply pruning wrapper with pruning policy parameter.\n",
    "try:\n",
    "    pruned_model = prune_low_magnitude(model, **pruning_params)\n",
    "    print('Success')\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=5e-6)\n",
    "pruned_model.compile(opt, loss=cce,\n",
    "                  metrics=['accuracy', ArgmaxMeanIOU(NUM_CLASSES)]) \n",
    "pruned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_qYnWrywU_G",
    "outputId": "95805612-375c-44b1-ecf8-8bc9cc7dd3ae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine-tuning on trained tf model\n",
    "import tempfile\n",
    "\n",
    "logdir = tempfile.mkdtemp()\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep()\n",
    "]   \n",
    "history = pruned_model.fit(train_ds,\n",
    "                validation_data=valid_ds,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "executionInfo": {
     "elapsed": 54407,
     "status": "error",
     "timestamp": 1626489786684,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "fLHOpm-tPsqZ",
    "outputId": "cb38c789-6c15-488c-e41e-f24f39c75b30",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pruned_model.save('E:/EE/project/FPGA/128x256/pruned_small_2_model.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk6osjXcMMRy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pruned_model = tf.keras.models.load_model(\"E:\\EE\\project\\FPGA\\pruned_model_SparsityDecay.tf\",custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU})\n",
    "\n",
    "stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "stripped_pruned_model_copy = tf.keras.models.clone_model(stripped_pruned_model)\n",
    "stripped_pruned_model_copy.set_weights(stripped_pruned_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_pruned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y81g1T5eMKYN"
   },
   "outputs": [],
   "source": [
    "def print_model_weights_sparsity(model):\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Wrapper):\n",
    "            weights = layer.trainable_weights\n",
    "        else:\n",
    "            weights = layer.weights\n",
    "        for weight in weights:\n",
    "            if \"kernel\" not in weight.name or \"centroid\" in weight.name:\n",
    "                continue\n",
    "            weight_size = weight.numpy().size\n",
    "            zero_num = np.count_nonzero(weight == 0)\n",
    "            print(\n",
    "                f\"{weight.name}: {zero_num/weight_size:.2%} sparsity \",\n",
    "                f\"({zero_num}/{weight_size})\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_weights_sparsity(stripped_pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stripped_pruned_model.save('E:/EE/project/FPGA/128x256/stripped_pruned_small_3_model.tf', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_pruned_model_copy = tf.keras.models.load_model('E:/EE/project/FPGA/128x256/stripped_pruned_small_2_model.tf',custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "executionInfo": {
     "elapsed": 4964,
     "status": "error",
     "timestamp": 1626450569176,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "4NZfByOEHTbl",
    "outputId": "4369cfea-2eee-4d68-c0cd-649603aa0993",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Clustering\n",
    "cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "CentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n",
    "\n",
    "#clustering_params = {\n",
    "#  'number_of_clusters': 8,\n",
    "#  'cluster_centroids_init': CentroidInitialization.KMEANS_PLUS_PLUS\n",
    "#}\n",
    "\n",
    "#clustered_model = cluster_weights(stripped_pruned_model, **clustering_params)\n",
    "\n",
    "#clustered_model.compile(sgd, loss=cce,\n",
    " #                 metrics=['accuracy', ArgmaxMeanIOU(NUM_CLASSES)]) \n",
    "\n",
    "#print('Train clustering model:')\n",
    "#clustered_model.fit(train_ds, validation_data=valid_dsvalid_ds, epochs=5)\n",
    "\n",
    "\n",
    "#stripped_pruned_model.save(\"/content/drive/MyDrive/Colab_Notebooks/stripped_pruned_model_clustered.tf\")\n",
    "\n",
    "\n",
    "# Sparsity preserving clustering\n",
    "from tensorflow_model_optimization.python.core.clustering.keras.experimental import (\n",
    "    cluster,\n",
    ")\n",
    "\n",
    "cluster_weights = cluster.cluster_weights\n",
    "\n",
    "clustering_params = {\n",
    "  'number_of_clusters': 8,\n",
    "  'cluster_centroids_init': CentroidInitialization.KMEANS_PLUS_PLUS,\n",
    "  'preserve_sparsity': True\n",
    "}\n",
    "\n",
    "sparsity_clustered_model = cluster_weights(stripped_pruned_model_copy, **clustering_params)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=5e-6)\n",
    "sparsity_clustered_model.compile(opt, loss=cce,\n",
    "                  metrics=['accuracy', ArgmaxMeanIOU(NUM_CLASSES)])\n",
    "\n",
    "print('Train sparsity preserving clustering model:')\n",
    "sparsity_clustered_model.fit(train_ds, validation_data=valid_ds, epochs=5)\n",
    "sparsity_clustered_model.save(\"E:/EE/project/FPGA/128x256/sparsity_clustered_small_2_model.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stripped_sparsity_clustered_model = tfmot.clustering.keras.strip_clustering(sparsity_clustered_model)\n",
    "stripped_sparsity_clustered_model.save(\"E:/EE/project/FPGA/128x256/stripped_sparsity_clustered_small_2_model.tf\",include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "aborted",
     "timestamp": 1626450568516,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "ibKD2PKEH_K2"
   },
   "outputs": [],
   "source": [
    "stripped_sparsity_clustered_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = tf.keras.models.load_model('E:/EE/project/FPGA/512x256/model8.tf',custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU})\n",
    "#stripped_pruned_model_copy = tf.keras.models.load_model('E:/EE/project/FPGA/512x256/stripped_pruned_model_SparsityDecay.tf',custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU})\n",
    "#stripped_sparsity_clustered_model = tf.keras.models.load_model('E:/EE/project/FPGA/128x256/stripped_sparsity_clustered_model.tf',custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU})\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=5e-6)\n",
    "\n",
    "stripped_pruned_model_copy.compile(opt, loss=cce,\n",
    "                  metrics=['accuracy', ArgmaxMeanIOU(NUM_CLASSES)])\n",
    "stripped_sparsity_clustered_model.compile(opt, loss=cce,\n",
    "                  metrics=['accuracy', ArgmaxMeanIOU(NUM_CLASSES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fps(model):\n",
    "    src = cv2.imread('E:/EE/project/FPGA/test_01.png')\n",
    "    start = time.time()\n",
    "    result = img_pred(src,model,(256,128))[2]\n",
    "    end = time.time()\n",
    "    fps = 1/(end-start)\n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate model on valid data\")\n",
    "results1 = m.evaluate(valid_ds)\n",
    "print(\"Evaluate pruned model on valid data\")\n",
    "results2 = stripped_pruned_model_copy.evaluate(valid_ds)\n",
    "print(\"Evaluate pruned clustered model on valid data\")\n",
    "results2 = stripped_sparsity_clustered_model.evaluate(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Original fps: ', test_fps(m))\n",
    "print('Pruned fps: ', test_fps(stripped_pruned_model_copy))\n",
    "print('Pruned Clustered fps: ', test_fps(stripped_sparsity_clustered_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "executionInfo": {
     "elapsed": 2281,
     "status": "error",
     "timestamp": 1626451207796,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "AZGg0pyR9yd3",
    "outputId": "8cd273bb-df63-4167-f65a-2928c440b377"
   },
   "outputs": [],
   "source": [
    "# Test Image\n",
    "src = cv2.imread('E:/EE/project/FPGA/test_01.png')\n",
    "start = time.time()\n",
    "result = img_pred(src,stripped_sparsity_clustered_model)[2]\n",
    "end = time.time()\n",
    "fps = 1/(end-start)\n",
    "print('fps =',fps)\n",
    "cv2.imshow(\"result\",result)\n",
    "cv2.waitKey (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84534,
     "status": "ok",
     "timestamp": 1626423933526,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "ZlK6Ytv_aiwv",
    "outputId": "339cb050-e9a1-4adc-e30b-cc3b900b1138",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quantize model to a tflite int8 model speed up for ARM\n",
    "\n",
    "model = tf.keras.models.load_model('E:EE/project/FPGA/128x256/stripped_sparsity_clustered_small_2_model.tf',custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU})\n",
    "#model = stripped_sparsity_clustered_model\n",
    "cityscapes = tfds.load('cityscapes/semantic_segmentation',data_dir=\"E:\\EE\\project\\FPGA\\cityscapes\",download=False)\n",
    "test_ds = cityscapes['test'].map(cityscapes_prep(OUTPUT_SHAPE, INPUT_SHAPE, class_map_road))\n",
    "\n",
    "def representative_dataset():\n",
    "  for data in test_ds.take(100).batch(1):\n",
    "      yield [data[0]]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#converter.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY]\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "#converter.inference_input_type = tf.uint8\n",
    "#converter.inference_output_type = tf.uint8\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "\n",
    "#tflite_quant_model_fullyINT8 = converter.convert()\n",
    "tflite_quant_model_fullyINT8 = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1626423946570,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "3WuMY0ORap8O",
    "outputId": "e7a5c838-773d-4a93-f3fc-a3057ca3d340"
   },
   "outputs": [],
   "source": [
    "# Check input/output dtype\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model_fullyINT8)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1626425592690,
     "user": {
      "displayName": "許晏誠",
      "photoUrl": "",
      "userId": "15153068038467638530"
     },
     "user_tz": -480
    },
    "id": "2VxzOuAxbwA7",
    "outputId": "fbee7b3e-5955-400f-e37d-70010f2db2dc"
   },
   "outputs": [],
   "source": [
    "# Save float and int tflite model\n",
    "import pathlib\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"E:/EE/project/FPGA/128x256/cityscapes_tflite_models\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "tflite_model_quant_fullINT_file = tflite_models_dir/\"cityscapes_f32_small_2_pc.tflite\"\n",
    "tflite_model_quant_fullINT_file.write_bytes(tflite_quant_model_fullyINT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ql_a9p02cwW_"
   },
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model with dataset\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "def run_tflite_model_valid_MIoU(tflite_file, test, size):\n",
    "    cnt = 0\n",
    "    for sample in test:\n",
    "        cnt += 1\n",
    "        gt = sample[1]\n",
    "        test_image = sample[0]\n",
    "  \n",
    "        gt = tf.argmax(gt, axis=-1)\n",
    "        gt = gt[..., tf.newaxis]\n",
    "  \n",
    "  # Initialize the interpreter\n",
    "        interpreter = tf.lite.Interpreter(model_path=str(tflite_file),num_threads=4)\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "        test_image = cv2.cvtColor(np.array(test_image), cv2.COLOR_BGR2RGB)\n",
    "        test_image = cv2.resize(test_image,size,interpolation=cv2.INTER_CUBIC)\n",
    "        image = test_image/255\n",
    "        data = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "        data = tf.expand_dims(data, axis=0)\n",
    "        interpreter.set_tensor(input_details[\"index\"], data)\n",
    "  \n",
    "        s = time.time()\n",
    "        interpreter.invoke()\n",
    "        e = time.time()\n",
    "        fps = 1/(e-s)\n",
    "        \n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "        seg = tf.argmax(output, axis=-1)\n",
    "        seg = seg[..., tf.newaxis]\n",
    "        m = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "        m.update_state(seg, gt)\n",
    "        \n",
    "        x = 'fps: '\n",
    "        text = \"{}{:.3f}\".format(x,fps)\n",
    "        y = \"MIoU: \"\n",
    "        text2 = \"{}{:.3f}\".format(y,m.result().numpy())\n",
    "        z = \"#\"\n",
    "        text3 = \"{}{}\".format(z,cnt)\n",
    "        \n",
    "        seg = tf.keras.preprocessing.image.array_to_img(seg)\n",
    "\n",
    "        seg = cv2.cvtColor(np.array(seg), cv2.COLOR_BGR2RGB)\n",
    "        result = cv2.addWeighted(test_image, 0.6, seg, 0.5, 0, dtype = cv2.CV_8U)\n",
    "        if size == (512,256):\n",
    "            result = cv2.resize(result,(512,256),interpolation=cv2.INTER_CUBIC)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(result, text , (100,250), font, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(result, text2 , (250,250), font, 0.7, (100, 100, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(result, text3 , (10,50), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('result',result)\n",
    "        if cv2.waitKey(300) & 0xFF == ord('q') or cnt == 100:\n",
    "            break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HopYCVroer8x",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (128, 256, 3)\n",
    "NUM_CLASSES = 2\n",
    "SCALE = 8\n",
    "OUTPUT_SHAPE = (128,256,2)\n",
    "\n",
    "cityscapes = tfds.load('cityscapes/semantic_segmentation',data_dir=\"E:\\EE\\project\\FPGA\\cityscapes\",download=False)\n",
    "test_ds = cityscapes['validation'].map(cityscapes_prep(OUTPUT_SHAPE, INPUT_SHAPE, class_map_road, float_range=False))\n",
    "path = \"E:/EE/project/FPGA/128x256/cityscapes_tflite_models/cityscapes_f32_small_2.tflite\"\n",
    "\n",
    "run_tflite_model_valid_MIoU(path,test_ds.shuffle(100),(512,256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ql_a9p02cwW_"
   },
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model with image\n",
    "import cv2\n",
    "\n",
    "def run_tflite_model(tflite_file, test_image):\n",
    "    \n",
    "  \n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file),num_threads=4)\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  test_image = cv2.cvtColor(np.array(test_image), cv2.COLOR_BGR2RGB)\n",
    "  test_image = cv2.resize(test_image,(512,256),interpolation=cv2.INTER_CUBIC)\n",
    "  image = test_image/255\n",
    "  data = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "  data = tf.expand_dims(data, axis=0)\n",
    "  interpreter.set_tensor(input_details[\"index\"], data)\n",
    "  \n",
    "  print('start')\n",
    "  s = time.time()\n",
    "  interpreter.invoke()\n",
    "  e = time.time()\n",
    "  print('end')  \n",
    "\n",
    "  print(f'invoke: {e-s:.3f}s ({1/(e-s):.2f} fps)')\n",
    "\n",
    "  output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "  seg = tf.argmax(output, axis=-1)\n",
    "  seg = seg[..., tf.newaxis]\n",
    "  seg = tf.keras.preprocessing.image.array_to_img(seg)\n",
    "\n",
    "  seg = cv2.cvtColor(np.array(seg), cv2.COLOR_BGR2RGB)\n",
    "  result = cv2.addWeighted(test_image, 0.6, seg, 0.5, 0, dtype = cv2.CV_8U)\n",
    "  \n",
    "  return src, seg, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('E:/EE/project/FPGA/test_01.png')\n",
    "path = \"E:/EE/project/FPGA/512x256/cityscapes_tflite_models/cityscapes_EXPiof32.tflite\"\n",
    "#path = \"E:/EE/project/FPGA/512x256/cityscapes_tflite_models/cityscapes_fullyINT8.tflite\"\n",
    "\n",
    "result = run_tflite_model(path,src)[2]\n",
    "\n",
    "cv2.imshow('result',result)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVZwZCwTiI-a"
   },
   "outputs": [],
   "source": [
    "# helper function to evaluate on the tf dataset\n",
    "import time \n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(path),num_threads=4)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "INPUT_SHAPE = output_details[0][\"shape\"][1:4]\n",
    "\n",
    "input_details\n",
    "\n",
    "OUTPUT_SHAPE = output_details[0][\"shape\"][1:4]\n",
    "\n",
    "output_details\n",
    "\n",
    "cityscapes = tfds.load('cityscapes/semantic_segmentation',data_dir=\"E:\\EE\\project\\FPGA\\cityscapes\",download=False)\n",
    "test_ds = cityscapes['validation'].map(cityscapes_prep(OUTPUT_SHAPE, INPUT_SHAPE, class_map_road, float_range=False))\n",
    "\n",
    "def pred_func_tflite(sample):\n",
    "    input_data = tf.expand_dims(sample[0], axis=0)\n",
    "    input_data = tf.cast(input_data, tf.dtypes.float32)\n",
    "    \n",
    "    #truth_data = sample[1]\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n",
    "    \n",
    "    print('start')\n",
    "    s = time.time()\n",
    "    interpreter.invoke()\n",
    "    e = time.time()\n",
    "    print('end')\n",
    "    \n",
    "    print(f'invoke: {e-s:.3f}s ({1/(e-s):.2f} fps)')\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    seg = tf.argmax(output_data[0], axis=-1)\n",
    "    seg = seg[..., tf.newaxis]\n",
    "    seg = tf.keras.preprocessing.image.array_to_img(seg)\n",
    "    #plt.imshow(seg)\n",
    "    seg = cv2.cvtColor(np.array(seg), cv2.COLOR_BGR2RGB)\n",
    "    result = cv2.addWeighted(src, 0.6, seg, 0.5, 0, dtype = cv2.CV_8U)\n",
    "    return input_data[0], output_data[0]\n",
    "\n",
    "display_dataset(test_ds.take(10), pred_func_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BiSeNetV2-TFKeras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
